#ë¼ì´ë¸ŒëŸ¬ë¦¬
##ê¸°ë³¸ ì‘ì—…
import os
import pandas as pd
import numpy as np
import time
import json
from datetime import datetime, timedelta
##ë‰´ìŠ¤ìŠ¤í¬ë©
import re
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from dateutil.parser import parse
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
## API êµ¬ë™
import http.client
import uuid
from supabase import create_client, Client
from google import genai
from google.generativeai import types



###########################################################
# 1. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ

def parse_time_ago(time_str):
    """
    'X minutes/hours ago' í˜•íƒœì˜ ì‹œê°„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ timedelta ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: "Reutersâ€¢13 hours ago" -> timedelta(hours=13)
    """
    # "Reutersâ€¢" ê°™ì€ ë°œí–‰ì‚¬ ì •ë³´ ì œê±°
    if 'â€¢' in time_str:
        time_str = time_str.split('â€¢')[1].strip()

    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìˆ«ìì™€ ì‹œê°„ ë‹¨ìœ„(minute, hour)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    match = re.search(r'(\d+)\s+(minute|hour)s?\s+ago', time_str, re.IGNORECASE)
    
    if match:
        value = int(match.group(1))
        unit = match.group(2).lower()
        
        if unit == 'minute':
            return timedelta(minutes=value)
        elif unit == 'hour':
            return timedelta(hours=value)
            
    return None

##ìƒˆë¡œìš´ ì›¹í¬ë¡¤ë§ í•¨ìˆ˜ ìƒì„±
def get_all_news_links(base_urls):
    """
    ì£¼ì–´ì§„ ëª¨ë“  URL í˜ì´ì§€ì—ì„œ 12ì‹œê°„ ì´ë‚´ì— ì‘ì„±ëœ
    ëª¨ë“  ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    unique_articles = {}

    # for ë£¨í”„ ì•ˆì—ì„œ ë§¤ë²ˆ ë“œë¼ì´ë²„ë¥¼ ìƒì„±í•˜ê³  ì¢…ë£Œí•˜ë„ë¡ êµ¬ì¡° ë³€ê²½
    for url in base_urls:
        print(f"'{url}' í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...")
        
        # Selenium WebDriver ì„¤ì •
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.binary_location = "/usr/bin/google-chrome"
        
        service = Service()
        driver = webdriver.Chrome(service=service, options=options)

        try:
            driver.get(url)
            wait = WebDriverWait(driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "li.stream-item.story-item")))

            # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì œí•œ (ìµœëŒ€ 10íšŒ)
            print("í˜ì´ì§€ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 10íšŒ)...")
            for i in range(10):
                try:
                    last_height = driver.execute_script("return document.body.scrollHeight")
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    WebDriverWait(driver, 15).until(
                        lambda d: d.execute_script("return document.body.scrollHeight") > last_height
                    )
                    print(f"ìŠ¤í¬ë¡¤ {i+1}/10 ì™„ë£Œ, ìƒˆ ì½˜í…ì¸  ë¡œë“œë¨.")
                except TimeoutException:
                    print("ë” ì´ìƒ ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ì–´ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
            
            # í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            news_list = soup.select('li.stream-item.story-item')

            for item in news_list:
                title_tag = item.select_one('h3')
                link_tag = item.select_one('a.subtle-link')
                time_tag = item.select_one('div.publishing')

                if title_tag and link_tag and link_tag.has_attr('href') and time_tag:
                    time_str = time_tag.get_text(strip=True)
                    time_delta = parse_time_ago(time_str)
                    
                    if time_delta and time_delta <= timedelta(hours=12):
                        title = title_tag.get_text(strip=True)
                        link = "https://finance.yahoo.com" + link_tag['href']

                        if title not in unique_articles:
                            unique_articles[title] = {"url": link, "time": time_str}

        except Exception as e:
            print(f"'{url}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            # try, exceptì™€ ìƒê´€ì—†ì´ ì‘ì—…ì´ ëë‚˜ë©´ ë°˜ë“œì‹œ ë“œë¼ì´ë²„ ì¢…ë£Œ
            driver.quit()

    # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
    article_list = [{"title": title, "url": data["url"], "time": data["time"]} for title, data in unique_articles.items()]
    print(f"ì´ {len(article_list)}ê°œì˜ 12ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return article_list
# ëŒ€ìƒ URL ëª©ë¡
target_urls = [
    "https://finance.yahoo.com/topic/latest-news/",
    "https://finance.yahoo.com/topic/stock-market-news/",
    "https://finance.yahoo.com/topic/yahoo-finance-originals/",
    "https://finance.yahoo.com/topic/economic-news/",
    # "https://finance.yahoo.com/topic/earnings/",
    # "https://finance.yahoo.com/topic/tech/",
    "https://finance.yahoo.com/topic/electric-vehicles/"
]

# í•¨ìˆ˜ ì‹¤í–‰
news_list = get_all_news_links(target_urls)

##############################################
# 2. ë‰´ìŠ¤ ìŠ¤í¬ë©

def get_article_details(article_list):
    """
    ë‰´ìŠ¤ ëª©ë¡ì„ ë°›ì•„ ê° ê¸°ì‚¬ì˜ ë³¸ë¬¸ê³¼ ê²Œì‹œ ë‚ ì§œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì¦‰, ê° ë‰´ìŠ¤ì˜ ë§í¬ì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ê³¼ ê²Œì‹œ ë‚ ì§œë¥¼ ì¶”ì¶œí•œë‹¤.
    """ 
    final_results = [] #ìµœì¢… ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    for i, article in enumerate(article_list):
        print(f"({i+1}/{len(article_list)}) '{article['title']}' ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...") #ì‘ì—… í™•ì¸ìš©
        
        #ì•ì„œ ìˆ˜ì§‘í•œ ë§í¬ëŠ” ì£¼ì†Œê°€ ì´ìƒí•˜ê²Œ ë˜ì–´ìˆì–´ì„œ ìˆ˜ì •í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤
        url_to_fetch = article['url']
        if url_to_fetch.count('https://finance.yahoo.com') > 1:
            url_to_fetch = 'https://finance.yahoo.com' + url_to_fetch.split('https://finance.yahoo.com')[-1]
        
        # ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘
        retries = 3 #ì‹¤íŒ¨í–ˆì„ ê²½ìš° ì„¸ë²ˆ ë°˜ë³µ
        for attempt in range(retries):
            try:
                response = requests.get(url_to_fetch, headers={'User-Agent': 'Mozilla/5.0'})
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser') #html íŒŒì‹±

                # 1. ë³¸ë¬¸ ì¶”ì¶œ
                content_parts = []
                body_wrappers = soup.select('div.atoms-wrapper, div.read-more-wrapper') #ë³¸ë¬¸ì€ ë‘ê°œì˜ íƒœê·¸ì— ì €ì¥ë˜ì–´ìˆë‹¤.
                for wrapper in body_wrappers:
                    content_parts.append(wrapper.get_text(separator=' ', strip=True)) #divì•ˆì— ì—¬ëŸ¬ê°œì˜ íƒœê·¸ë¡œ ë³¸ë¬¸ ë‹¨ë½ì´ ë¶„ë¦¬ë˜ì–´ìˆë‹¤. ë”°ë¼ì„œ ì´ê²ƒì„ í•©ì¹˜ëŠ” ê³¼ì •ì´ë‹¤.
                content = ' '.join(content_parts)

                # 2. ê²Œì‹œ ë‚ ì§œ ì¶”ì¶œ ë° í˜•ì‹ ë³€í™˜
                time_tag = soup.select_one('time')
                publish_date_str = "N/A"
                if time_tag and time_tag.has_attr('datetime'):
                    dt_object = parse(time_tag['datetime'])
                    publish_date_str = dt_object.strftime('%Y-%m-%d %H:%M')

                # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                final_results.append({
                    "title": article['title'], #ì œëª©
                    "publish_date": publish_date_str, #ê²Œì‹œ ë‚ ì§œ
                    "url": url_to_fetch, #ê¸°ì‚¬ ë§í¬
                    "content": content #ë³¸ë¬¸ ë‚´ìš©
                })
                
                time.sleep(2) # ì„±ê³µ ì‹œ ìš”ì²­ ê°„ê²©ì„ 2ì´ˆë¡œ ëŠ˜ë¦¼
                break # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ

            # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ê²½ìš° ì¬ì‹œë„
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    wait_time = 10
                    print(f"  [ì•Œë¦¼] 429 ì˜¤ë¥˜ ë°œìƒ. {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt + 1}/{retries})")
                    time.sleep(wait_time)
                else:
                    print(f"  [ì˜¤ë¥˜] HTTP ì˜¤ë¥˜ ë°œìƒ: {e}")
                    break # ë‹¤ë¥¸ HTTP ì˜¤ë¥˜ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
            except requests.exceptions.RequestException as e:
                print(f"  [ì˜¤ë¥˜] ê¸°ì‚¬ ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break # ì¼ë°˜ì ì¸ ìš”ì²­ ì˜¤ë¥˜ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
            except Exception as e:
                print(f"  [ì˜¤ë¥˜] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                break
        else: # for ë£¨í”„ê°€ break ì—†ì´ ëë‚¬ì„ ê²½ìš° (ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨)
            print(f"  [ì‹¤íŒ¨] ëª¨ë“  ì¬ì‹œë„ì— ì‹¤íŒ¨í•˜ì—¬ ë‹¤ìŒ ê¸°ì‚¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
            
    return final_results

# í•¨ìˆ˜ ì‹¤í–‰
# ì´ì „ì— ìƒì„±ëœ news_listë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
full_news_data = get_article_details(news_list)

# ìµœì¢… ê²°ê³¼ ìƒì„± (ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜)
df = pd.DataFrame(full_news_data)
# df['publish_date'] = pd.to_datetime(df['publish_date'], errors='coerce') + pd.Timedelta(hours=9) # í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ UTC+9
print("--- ìµœì¢… ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")

###############################################################
# 3. ë‰´ìŠ¤ ìš”ì•½
# --- ì—¬ê¸°ì— API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš” ---
API_KEY = os.environ.get("GEMINI_API_KEY")

def analyze_news_article(article_text: str, api_key: str) -> str:
    """
    í•˜ë‚˜ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ Gemini APIë¡œ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    ë³´ë‚´ì£¼ì‹  ê³µì‹ ì˜ˆì œ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¦…ë‹ˆë‹¤.
    """
    try:
        # 1. genai.Clientë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        client = genai.Client(api_key=api_key)

        # 2. ëª¨ë¸ ì´ë¦„ì„ ìš”ì²­í•˜ì‹  ê·¸ëŒ€ë¡œ "gemini-2.5-flash"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        model = "gemini-2.5-flash"
        
        # 3. types.Contentì™€ types.Partë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ë‚´ìš©ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text="""You are an expert stock analyst. Your task is to analyze an English news article and provide a summary.

### INSTRUCTIONS ###
1.  Analyze the provided English news article for key information relevant to investors (e.g., corporate earnings, new products, M&A, regulatory changes).
2.  Create a three-sentence summary in English.
3.  The summary must explicitly state whether the nuance of the news is positive, negative, or neutral for investors.

### EXAMPLES & BEHAVIORAL RULES ###
This is an example of how to behave.

**Example Scenario:** If the user provides instructions but no article to analyze below the "---" line.
**Your Correct Response in this case is this plain text:**
To provide an expert analysis and a three-sentence summary, the news article must be provided. Please submit the English news article for review, and I will extract the key investor-relevant information, determine the market nuance, and synthesize it into the requested format. Without the article, a specific analysis is not possible.

---
ARTICLE TO ANALYZE:
---

[ì—¬ê¸°ì— ë¶„ì„í•  ì˜ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”]"""),
            ],
            ),
            types.Content(
                role="model",
                parts=[
                    types.Part.from_text(text="""{"summary": "Understood. I am ready to analyze the provided news article."}"""),
                ],
            ),
            types.Content(
                role="user",
                parts=[
                    # í•¨ìˆ˜ ì¸ìë¡œ ë°›ì€ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì—¬ê¸°ì— ì‚½ì…í•©ë‹ˆë‹¤.
                    types.Part.from_text(text=article_text),
                ],
            ),
        ]
        
        # 4. client.models.generate_content_streamì„ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
        response_chunks = []
        for chunk in client.models.generate_content_stream(
            model=model,
            contents=contents,
        ):
            response_chunks.append(chunk.text)
        
        return "".join(response_chunks)

    except Exception as e:
        print(f"An error occurred: {e}")
        return '{"summary": "Error during analysis."}'


# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":

    df['summary'] = df['content'].apply(lambda content: analyze_news_article(content, api_key=API_KEY))

############################################3
# 4. ë²¡í„°í™” (ì„ë² ë”©)

def get_summary_embedding(summary_text: str, client: genai.Client) -> list[float] | None:
    """
    í•˜ë‚˜ì˜ ìš”ì•½ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„ë² ë”© ë²¡í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    """
    # 1. ìš”ì•½ë³¸ ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if not summary_text or pd.isna(summary_text):
        return None
    
    try:
        # 2. 'contents'ê°€ ì•„ë‹Œ 'content' íŒŒë¼ë¯¸í„°ë¡œ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
        result = client.models.embed_content(
            model="models/text-embedding-004",
            contents=summary_text,
            config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
        )
        # 3. ê²°ê³¼ ê°ì²´ì—ì„œ .embedding ì†ì„±ìœ¼ë¡œ ë²¡í„°ë¥¼ ì§ì ‘ ë°˜í™˜
        vectors = [obj.values for obj in result.embeddings]
        vectors=vectors[0]

        return vectors
    
    except Exception as e:
        print(f"API Error embedding '{summary_text[:50]}...': {e}")
        return None


# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    # í´ë¼ì´ì–¸íŠ¸ëŠ” í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤.
    client = genai.Client(api_key=API_KEY)
    df['embedding'] = df['summary'].apply(lambda text: get_summary_embedding(text, client))

#################################################3
# 6. Supabase ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë¡œë“œ

supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_KEY")

try:
    supabase: Client = create_client(supabase_url, supabase_key)
    print("â˜ï¸ Supabaseì— ì—°ê²°í•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # DBì— ì €ì¥ëœ ëª¨ë“  'title' ëª©ë¡ì„ ê°€ì ¸ì˜¤ê¸°
    response = supabase.table('financial_news_summary').select('title').execute()
    existing_titles = {item['title'] for item in response.data}
    print(f"í˜„ì¬ DBì— {len(existing_titles)}ê°œì˜ ê¸°ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤.")

    # ìƒˆë¡œ ìƒì„±ëœ DataFrameì—ì„œ ì´ë¯¸ ìˆëŠ” titleë“¤ì„ ì œì™¸
    new_articles_df = df[~df['title'].isin(existing_titles)]

    if new_articles_df.empty:
        print("âœ… ì¶”ê°€í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ¨ {len(new_articles_df)}ê°œì˜ ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.")

    
        # ì—…ë¡œë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        records_to_upload = new_articles_df.to_dict('records')
        for record in records_to_upload:
            record['publish_date'] = record['publish_date'].isoformat()
        
        # ë°ì´í„° ì‚½ì…
        data, count = supabase.table('financial_news_summary').insert(records_to_upload).execute()
        print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ {len(data[1])}ê°œì˜ ìƒˆ ê¸°ì‚¬ë¥¼ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"âŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
