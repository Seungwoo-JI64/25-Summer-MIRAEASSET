# analysis_model/agents/market_correlation_agent.py

import pandas as pd
from typing import Dict, Any, List, Set
from datetime import datetime, timedelta

from ..state import AnalysisState, MarketAnalysisResult, NewsImpactData, TickerPriceData
from .data_prep_agent import supabase_client
from .news_analyst_agent import METRICS_MAP # METRICS_MAPì´ ì´ íŒŒì¼ì—ì„œ í•„ìš”í•©ë‹ˆë‹¤.

def get_correlation_text(kor_name: str, metric_name: str, corr_value: float) -> str:
    """ìƒê´€ê³„ìˆ˜ ê°’ì— ë”°ë¼ í•´ì„ì„ ë‹´ì€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if corr_value is None:
        return f"'{kor_name}'ê³¼(ì™€) '{metric_name}'ì˜ ìƒê´€ê´€ê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    corr_value = round(corr_value, 2)
    if corr_value > 0.7: relation_text = "ë§¤ìš° ê°•í•œ ì–‘ì˜ ê´€ê³„"
    elif corr_value > 0.3: relation_text = "ì–´ëŠ ì •ë„ ëšœë ·í•œ ì–‘ì˜ ê´€ê³„"
    elif corr_value > -0.3: relation_text = "ê±°ì˜ ê´€ê³„ê°€ ì—†ê±°ë‚˜ ë§¤ìš° ì•½í•œ ê´€ê³„"
    elif corr_value > -0.7: relation_text = "ì–´ëŠ ì •ë„ ëšœë ·í•œ ìŒì˜ ê´€ê³„"
    else: relation_text = "ë§¤ìš° ê°•í•œ ìŒì˜ ê´€ê³„"
    return f"'{kor_name}'ê³¼(ì™€) '{metric_name}'ì˜ ìƒê´€ê³„ìˆ˜ëŠ” {corr_value}ë¡œ, '{relation_text}'ë¥¼ ë³´ì…ë‹ˆë‹¤."

def get_stock_data_from_supabase(ticker: str, start_date_str: str, end_date_str: str) -> pd.DataFrame | None:
    """Supabase DBì—ì„œ íŠ¹ì • ê¸°ê°„ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    
    table_name, time_col, price_col, ticker_filter_col = "", "time", "close_price", "ticker" # Default values

    if ticker.startswith('^') or ticker.endswith('=X'):
        table_name = "financial_indices"
        time_col = "date"
        price_col = "value"
        ticker_filter_col = "index_en"
    elif ticker.endswith('.KS') or ticker.endswith('.KQ'): # Korean stocks
        table_name = "korean_stocks"
        time_col = "time" # Corrected: 'date' ëŒ€ì‹  'time' ì‚¬ìš©
        price_col = "close_price"
        ticker_filter_col = "ticker"
    else: # US stocks
        table_name = "us_stocks"
        time_col = "time" # Corrected: 'date' ëŒ€ì‹  'time' ì‚¬ìš©
        price_col = "close_price"
        ticker_filter_col = "ticker"

    try:
        res = supabase_client.table(table_name).select(f"{time_col}, {price_col}").eq(ticker_filter_col, ticker).gte(time_col, start_date_str).lte(time_col, end_date_str).order(time_col, desc=False).execute()
        
        if not res.data:
            return None
            
        df = pd.DataFrame(res.data)
        df.rename(columns={price_col: 'close', time_col: 'date'}, inplace=True) 

        df['date'] = pd.to_datetime(df['date']).dt.date
        df['date'] = df['date'].astype(str)

        df.drop_duplicates(subset=['date'], keep='last', inplace=True)

        return df
    except Exception as e:
        print(f"    âš ï¸ Supabaseì—ì„œ '{ticker}' ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def run_market_correlation(state: AnalysisState) -> Dict[str, Any]:
    print("\n--- ğŸ“ˆ ì‹œì¥ ìƒê´€ê´€ê³„ ë° ë‰´ìŠ¤ ì˜í–¥ ë¶„ì„ ì—ì´ì „íŠ¸ ì‹¤í–‰ ---")
    
    target_ticker = state.get("ticker")
    target_name = state.get("company_name")
    selected_news = state.get("selected_news", [])
    selected_domestic_news = state.get("selected_domestic_news", [])

    if not all([target_ticker, target_name]):
        print("    âš ï¸ í‹°ì»¤ ë˜ëŠ” íšŒì‚¬ ì´ë¦„ì´ ì—†ì–´ ì‹œì¥ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ê±´ë„ˆëœœ.")
        return {}

    # 1. ëª¨ë“  ê´€ë ¨ í‹°ì»¤ ì·¨í•© ë° ë‰´ìŠ¤ ë‚ ì§œ ì¶”ì¶œ (ë‰´ìŠ¤ URL, ì œëª© í¬í•¨)
    all_analyzed_tickers: Set[str] = set()
    # news_event_markers: { ticker: { 'YYYY-MM-DD': [ {title: '...', url: '...'}, {title: '...', url: '...'} ] } }
    news_event_markers_detailed: Dict[str, Dict[str, List[Dict[str, str]]]] = {}

    all_analyzed_tickers.add(target_ticker)

    for news_list in [selected_news, selected_domestic_news]:
        if news_list:
            for news_item in news_list:
                news_date = news_item.get("published_date") or news_item.get("publish_date")
                if news_date:
                    news_date_str = pd.to_datetime(news_date).strftime('%Y-%m-%d')
                    
                    # ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                    news_details = {
                        "title": news_item.get("title", "ì œëª© ì—†ìŒ"),
                        "url": news_item.get("url", "#"),
                        "date": news_date_str # ë‚ ì§œë¥¼ ì—¬ê¸°ì— ë‹¤ì‹œ í¬í•¨ì‹œì¼œ í´ë¦­ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œ í™œìš©
                    }

                    # ê´€ë ¨ í‹°ì»¤ì— ë‰´ìŠ¤ ì •ë³´ ì¶”ê°€
                    if news_item.get("related_metrics"):
                        for ticker in news_item["related_metrics"]:
                            all_analyzed_tickers.add(ticker)
                            news_event_markers_detailed.setdefault(ticker, {}).setdefault(news_date_str, []).append(news_details)
                    
                    # ëŒ€ìƒ í‹°ì»¤ì—ë„ ë‰´ìŠ¤ ì •ë³´ ì¶”ê°€
                    news_event_markers_detailed.setdefault(target_ticker, {}).setdefault(news_date_str, []).append(news_details)
    
    # ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (ë™ì¼ í‹°ì»¤, ë™ì¼ ë‚ ì§œ, ë™ì¼ URL ê¸°ì¤€)
    # ê° í‹°ì»¤-ë‚ ì§œë³„ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ setìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ listë¡œ
    cleaned_news_event_markers = {}
    for ticker, dates_dict in news_event_markers_detailed.items():
        cleaned_news_event_markers[ticker] = {}
        for date_str, news_items in dates_dict.items():
            # (title, url) íŠœí”Œì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
            unique_news_tuples = { (item['title'], item['url']) for item in news_items }
            # ì›ë³¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë‹¤ì‹œ ë³€í™˜
            cleaned_news_items = [{"title": t, "url": u, "date": date_str} for t, u in unique_news_tuples]
            cleaned_news_event_markers[ticker][date_str] = cleaned_news_items
    
    print(f"ë¶„ì„ ëŒ€ìƒ ì „ì²´ ê³ ìœ  ì§€í‘œ: {all_analyzed_tickers}")
    
    # 2. íˆìŠ¤í† ë¦¬ ë°ì´í„° ì¡°íšŒ (ìµœëŒ€ 2ë…„)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365 * 2)
    fetch_start_date_str = start_date.strftime('%Y-%m-%d')
    fetch_end_date_str = end_date.strftime('%Y-%m-%d')

    historical_prices_data: Dict[str, List[Dict[str, Any]]] = {}
    for ticker in all_analyzed_tickers:
        df = get_stock_data_from_supabase(ticker, fetch_start_date_str, fetch_end_date_str)
        if df is not None and not df.empty:
            historical_prices_data[ticker] = df.to_dict(orient='records')
        else:
            print(f"    -> '{ticker}'ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 3. ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° (ë³€í™” ì—†ìŒ)
    correlation_matrix_data: Dict[str, Dict[str, float]] = {}
    if len(all_analyzed_tickers) > 1:
        df_all_prices = pd.DataFrame()
        for ticker, data_list in historical_prices_data.items():
            if data_list:
                df_temp = pd.DataFrame(data_list)
                df_temp['date'] = pd.to_datetime(df_temp['date'])
                df_temp = df_temp.set_index('date')['close'].rename(ticker)
                df_all_prices = pd.concat([df_all_prices, df_temp], axis=1)
        
        df_all_prices = df_all_prices.ffill().bfill()
        
        if not df_all_prices.empty and len(df_all_prices.columns) > 1:
            correlation_matrix_data = df_all_prices.corr().to_dict(orient='index')
        else:
            print("    âš ï¸ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ê³„ì‚°í•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 4. ê¸°ì¡´ ìƒê´€ê´€ê³„ ìš”ì•½ (í…ìŠ¤íŠ¸) (ë³€í™” ì—†ìŒ)
    ticker_to_name_map = {target_ticker: target_name, **{t: i['name'] for t, i in METRICS_MAP.items()}}
    correlation_summary: List[str] = []
    _all_related_metrics_for_summary: Set[str] = set()
    for news in selected_news:
        _all_related_metrics_for_summary.update(news.get("related_metrics", []))
    for news in selected_domestic_news:
        _all_related_metrics_for_summary.update(news.get("related_metrics", []))

    for metric_ticker in _all_related_metrics_for_summary:
        metric_name = ticker_to_name_map.get(metric_ticker, metric_ticker)
        correlation = None
        try:
            if metric_ticker.startswith('^') or metric_ticker.endswith('=X'):
                res = supabase_client.table("correlation_kor_index").select("correlation").eq("ticker", target_ticker).eq("index_en", metric_ticker).execute()
                if res.data: correlation = res.data[0].get("correlation")
            else:
                res = supabase_client.table("correlation_kor_us").select("correlation").eq("korean_ticker", target_ticker).eq("us_ticker", metric_ticker).execute()
                if res.data: correlation = res.data[0].get("correlation")
            summary_text = get_correlation_text(target_name, metric_name, correlation)
            correlation_summary.append(summary_text)
        except Exception as e:
            print(f"âš ï¸ '{metric_ticker}' ìƒê´€ê´€ê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            correlation_summary.append(get_correlation_text(target_name, metric_name, None))

    # # 5. ë‰´ìŠ¤ ë¸”ë¡ë³„ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
    # all_news = selected_news + selected_domestic_news
    # if not all_news:
    #     final_market_result: MarketAnalysisResult = {
    #         "correlation_summary": correlation_summary,
    #         "news_impact_data": [],
    #         "correlation_matrix": correlation_matrix_data
    #     }
    #     return {
    #         "market_analysis_result": final_market_result,
    #         "historical_prices": historical_prices_data,
    #         "news_event_markers": cleaned_news_event_markers, # ë³€ê²½ëœ news_event_markers ë°˜í™˜
    #         "all_analyzed_tickers": list(all_analyzed_tickers)
    #     }
        
    # sorted_news = sorted(all_news, key=lambda x: x['publish_date'] if 'publish_date' in x else x['published_date'])
    
    # news_blocks: List[List[Dict]] = []
    # if sorted_news:
    #     current_block = [sorted_news[0]]
    #     for i in range(1, len(sorted_news)):
    #         prev_date_str = current_block[-1].get('publish_date') or current_block[-1].get('published_date')
    #         curr_date_str = sorted_news[i].get('publish_date') or sorted_news[i].get('published_date')

    #         if not prev_date_str or not curr_date_str:
    #             continue

    #         prev_date = pd.to_datetime(prev_date_str)
    #         curr_date = pd.to_datetime(curr_date_str)

    #         if (curr_date - prev_date).days <= 7:
    #             current_block.append(sorted_news[i])
    #         else:
    #             news_blocks.append(current_block)
    #             current_block = [sorted_news[i]]
    #     news_blocks.append(current_block)

    # news_impact_data: List[NewsImpactData] = []
    # for block in news_blocks:
    #     block_dates = []
    #     for n in block:
    #         news_date_str = n.get("publish_date") or n.get("published_date")
    #         if news_date_str:
    #             block_dates.append(pd.to_datetime(news_date_str).date())
        
    #     if not block_dates: continue

    #     block_start_raw = min(block_dates)
    #     block_end_raw = max(block_dates)

    #     fetch_start_date = (datetime.combine(block_start_raw, datetime.min.time()) - timedelta(days=7)).strftime('%Y-%m-%d')
    #     fetch_end_date = (datetime.combine(block_end_raw, datetime.min.time()) + timedelta(days=7)).strftime('%Y-%m-%d')
        
    #     block_tickers = {target_ticker}.union(*(set(n.get("related_metrics", [])) for n in block))
    #     block_titles = [n['title'] for n in block]
        
    #     price_data_by_name: Dict[str, TickerPriceData] = {}
    #     for ticker in block_tickers:
    #         df = get_stock_data_from_supabase(ticker, fetch_start_date, fetch_end_date)
    #         if df is not None and not df.empty:
    #             prices_list = [[row['date'], row['close']] for _, row in df.iterrows()]
    #             change_summary = "ë°ì´í„° ë¶€ì¡±"
    #             if len(df['close']) > 1:
    #                 start_price, end_price = df['close'].iloc[0], df['close'].iloc[-1]
    #                 percentage_change = ((end_price - start_price) / start_price) * 100 if start_price != 0 else 0
    #                 change_text = "ìƒìŠ¹" if percentage_change >= 0 else "í•˜ë½"
    #                 period_days = (datetime.strptime(df['date'].iloc[-1], '%Y-%m-%d').date() - datetime.strptime(df['date'].iloc[0], '%Y-%m-%d').date()).days + 1
    #                 change_summary = f"{period_days}ì¼ê°„ ì•½ {abs(percentage_change):.2f}% {change_text}í–ˆìŠµë‹ˆë‹¤."
                
    #             name = ticker_to_name_map.get(ticker, ticker)
    #             price_data_by_name[name] = {"ticker": ticker, "prices": prices_list, "change_summary": change_summary}
        
    #     if price_data_by_name:
    #         news_impact_data.append({
    #             "news_titles": block_titles, "start_date": fetch_start_date,
    #             "end_date": fetch_end_date, "price_data_by_name": price_data_by_name
    #         })

    # 5. ë‰´ìŠ¤ ë¸”ë¡ë³„ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    all_news = (selected_news) + (selected_domestic_news)
    news_impact_data: List[NewsImpactData] = []
    
    if all_news:
        sorted_news = sorted(all_news, key=lambda x: x.get('publish_date') or x.get('published_date'))
        news_blocks: List[List[Dict]] = []
        if sorted_news:
            current_block = [sorted_news[0]]
            for i in range(1, len(sorted_news)):
                prev_date = pd.to_datetime(current_block[-1].get('publish_date') or current_block[-1].get('published_date'))
                curr_date = pd.to_datetime(sorted_news[i].get('publish_date') or sorted_news[i].get('published_date'))
                if (curr_date - prev_date).days <= 7:
                    current_block.append(sorted_news[i])
                else:
                    news_blocks.append(current_block)
                    current_block = [sorted_news[i]]
            news_blocks.append(current_block)

        for block in news_blocks:
            block_dates = [pd.to_datetime(n.get('publish_date') or n.get('published_date')).date() for n in block]
            block_start_raw, block_end_raw = min(block_dates), max(block_dates)
            fetch_start_date = (datetime.combine(block_start_raw, datetime.min.time()) - timedelta(days=7)).strftime('%Y-%m-%d')
            fetch_end_date = (datetime.combine(block_end_raw, datetime.min.time()) + timedelta(days=7)).strftime('%Y-%m-%d')
            block_tickers = {target_ticker}.union(*(set(n.get("related_metrics", [])) for n in block))
            block_titles = [n['title'] for n in block]
            
            price_data_by_name: Dict[str, TickerPriceData] = {}
            for ticker in block_tickers:
                df = get_stock_data_from_supabase(ticker, fetch_start_date, fetch_end_date)
                if df is not None and not df.empty:
                    prices_list = [[row['date'], row['close']] for _, row in df.iterrows()]
                    change_summary = "ë°ì´í„° ë¶€ì¡±"
                    if len(df['close']) > 1:
                        start_price, end_price = df['close'].iloc[0], df['close'].iloc[-1]
                        percentage_change = ((end_price - start_price) / start_price) * 100 if start_price != 0 else 0
                        change_text = "ìƒìŠ¹" if percentage_change >= 0 else "í•˜ë½"
                        period_days = (pd.to_datetime(df['date'].iloc[-1]).date() - pd.to_datetime(df['date'].iloc[0]).date()).days + 1
                        change_summary = f"{period_days}ì¼ê°„ ì•½ {abs(percentage_change):.2f}% {change_text}í–ˆìŠµë‹ˆë‹¤."
                    
                    name = ticker_to_name_map.get(ticker, ticker)
                    price_data_by_name[name] = {"ticker": ticker, "prices": prices_list, "change_summary": change_summary}
            
            if price_data_by_name:
                news_impact_data.append({
                    "news_titles": block_titles, "start_date": fetch_start_date,
                    "end_date": fetch_end_date, "price_data_by_name": price_data_by_name
                })

    # 6. "ë‹¨ê¸° ì£¼ì‹ ë³€ë™" ê·¸ë˜í”„ë¥¼ ìœ„í•œ ë°ì´í„° ê°€ê³µ
    short_term_prices: Dict[str, List[Dict[str, Any]]] = {}
    temp_short_term_data: Dict[str, Dict[str, float]] = {}

    for impact_item in news_impact_data:
        for name, price_data in impact_item.get("price_data_by_name", {}).items():
            ticker = price_data["ticker"]
            if ticker not in temp_short_term_data:
                temp_short_term_data[ticker] = {}
            
            for date, close_price in price_data["prices"]:
                temp_short_term_data[ticker][date] = close_price

    for ticker, date_price_map in temp_short_term_data.items():
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        sorted_prices = sorted(date_price_map.items())
        short_term_prices[ticker] = [{"date": date, "close": price} for date, price in sorted_prices]

    final_market_result: MarketAnalysisResult = {
        "correlation_summary": correlation_summary,
        "news_impact_data": news_impact_data,
        "correlation_matrix": correlation_matrix_data
    }
    
    print("âœ… ì‹œì¥ ìƒê´€ê´€ê³„ ë° ë‰´ìŠ¤ ì˜í–¥ ë¶„ì„ ì™„ë£Œ.")
    return {
        "market_analysis_result": final_market_result,
        "historical_prices": historical_prices_data,
        "short_term_prices": short_term_prices, # ë‹¨ê¸° ë°ì´í„° ì¶”ê°€
        "news_event_markers": cleaned_news_event_markers, # ë³€ê²½ëœ news_event_markers ë°˜í™˜
        "all_analyzed_tickers": list(all_analyzed_tickers)
    }