import os
import pandas as pd
import numpy as np
import time
import json
from datetime import timedelta
import datetime
##ë‰´ìŠ¤ìŠ¤í¬ë©
import re
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from dateutil.parser import parse
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
## API êµ¬ë™
import http.client
import uuid
from supabase import create_client, Client
from google import genai
from google.genai import types
import pytz

##############################
# 1. ì—°í•©ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§

def crawl_yesterday_news():
    """
    ì—°í•©ë‰´ìŠ¤ êµ­ë‚´ì¦ì‹œ ì„¹ì…˜ì—ì„œ ì–´ì œ ì‘ì„±ëœ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # 1. ë‚ ì§œ ë° ì‹œê°„ëŒ€ ì„¤ì •
    # í•œêµ­ ì‹œê°„ëŒ€(UTC+9)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    tz = pytz.timezone('Asia/Seoul')
    
    # ì „ë‚  ë‰´ìŠ¤ë§Œì„ ì¶”ì¶œí•œë‹¤
    # ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¤‘ë³µ ì œê±°í•˜ëŠ”ê²ƒì€ ìì›ì´ ë§ì´ ì†Œëª¨ë¨
    # ì–´ì œ ë‚ ì§œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    yesterday = (datetime.datetime.now(tz) - datetime.timedelta(days=1)).date()
    current_year = yesterday.year
    
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {yesterday.strftime('%Y-%m-%d')}")

    # 2. ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    news_data = []
    
    # 3. ì›¹ í˜ì´ì§€ ìˆœíšŒ ë° ë°ì´í„° ì¶”ì¶œ
    MAX_PAGES_TO_CRAWL = 10  # í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    stop_crawling = False # í¬ë¡¤ë§ ì¤‘ë‹¨ í”Œë˜ê·¸

    for page_num in range(1, MAX_PAGES_TO_CRAWL + 1): # ê° í˜ì´ì§€ë³„ë¡œ
        if stop_crawling: # ì–´ì œ ë‚ ì§œë³´ë‹¤ ì˜¤ë˜ëœ ê¸°ì‚¬ê°€ ì¡´ì¬í•œë‹¤ë©´ ê·¸ í˜ì´ì§€ê¹Œì§€ ì§„í–‰
            break

        # ì—°í•©ë‰´ìŠ¤ êµ­ë‚´ì¦ì‹œ
        url = f'https://www.yna.co.kr/market-plus/domestic-stock/{page_num}'
        print(f"í¬ë¡¤ë§ ì‹œë„... í˜ì´ì§€: {page_num}")
        
        try:
            # í¬ë¡¤ë§ user-agent ì„¤ì •
            headers = { 
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê¸°ì‚¬ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” div.news-con íƒœê·¸ë¥¼ ëª¨ë‘ ì„ íƒ
            article_containers = soup.select('div.news-con')
            
            # í˜„ì¬ í˜ì´ì§€ì— ê¸°ì‚¬ ì»¨í…Œì´ë„ˆê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            if not article_containers:
                print(f"í˜ì´ì§€ {page_num}ì—ì„œ ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ(div.news-con)ë¥¼ ì°¾ì§€ ëª»í•´ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

            for article_con in article_containers:
                time_tag = article_con.select_one('span.txt-time') # ë‚ ì§œ
                title_anchor = article_con.select_one('a.tit-news') # ì œëª© ë° ë§í¬

                # í•„ìˆ˜ ìš”ì†Œê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ê¸°ì‚¬ëŠ” ê±´ë„ˆë›°ê¸° (ì˜¤ë¥˜ ë˜ëŠ” ê´‘ê³ )
                if not time_tag or not title_anchor:
                    continue
                
                time_str = time_tag.get_text(strip=True)
                
                # 4. ë‚ ì§œ í•„í„°ë§
                # ì–´ì œì— í•´ë‹¹ë˜ëŠ” ë‚ ì§œë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•´
                try:
                    # ê¸°ì‚¬ ë‚ ì§œ
                    # ì—°ë„, ì›”, ì¼ê¹Œì§€ë§Œ ì¶”ì¶œ
                    article_date = datetime.datetime.strptime(f"{current_year}-{time_str}", '%Y-%m-%d %H:%M').date()
                    
                    # ê¸°ì‚¬ ë‚ ì§œê°€ ì–´ì œì™€ ê°™ìœ¼ë©´ ë°ì´í„° ì¶”ê°€
                    if article_date == yesterday:
                        title_span = title_anchor.select_one('span.title01')
                        # ì œëª© íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
                        if not title_span:
                            continue
                            
                        title = title_span.get_text(strip=True)
                        
                        # ì œëª©ì´ '[ì†ë³´]'ë¡œ ì‹œì‘í•˜ëŠ” ê¸°ì‚¬ëŠ” ê±´ë„ˆë›°ê¸°.
                        # ì´ê²ƒì€ ë‚´ìš©ì´ ì—†ë‹¤
                        if title.startswith('[ì†ë³´]'):
                            continue
                        
                        link = title_anchor['href']
                        
                        datetime_for_db = datetime.datetime.strptime(f"{current_year}-{time_str}", '%Y-%m-%d %H:%M').strftime('%Y-%m-%d %H:%M:%S')
                        
                        news_data.append({
                            'title': title,
                            'date': datetime_for_db,
                            'url': link
                        })
                    
                    # ê¸°ì‚¬ ë‚ ì§œê°€ ì–´ì œë³´ë‹¤ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ëª¨ë“  í˜ì´ì§€ íƒìƒ‰ ì¤‘ë‹¨
                    elif article_date < yesterday:
                        stop_crawling = True
                        break # í˜„ì¬ í˜ì´ì§€ì˜ ë‚˜ë¨¸ì§€ ê¸°ì‚¬ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨

                except ValueError:
                    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ í•´ë‹¹ ê¸°ì‚¬ëŠ” ê±´ë„ˆë›°ê¸°
                    continue
            
            if stop_crawling:
                print(f"í˜ì´ì§€ {page_num}ì—ì„œ ëŒ€ìƒ ë‚ ì§œë³´ë‹¤ ì˜¤ë˜ëœ ê¸°ì‚¬ë¥¼ ë°œê²¬í•˜ì—¬ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

        except requests.exceptions.RequestException as e:
            print(f"í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url}, ì˜¤ë¥˜: {e}")
            break
            
    # 5. ê²°ê³¼ ì €ì¥ìš© ë°ì´í„°í”„ë ˆì„ ìƒì„±
    if not news_data:
        print("ìµœëŒ€ í˜ì´ì§€ê¹Œì§€ íƒìƒ‰í–ˆì§€ë§Œ ì–´ì œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
        
    df = pd.DataFrame(news_data)
    
    return df

# 6. ì›¹í¬ë¡¤ë§ í•¨ìˆ˜ ì‹¤í–‰
news_df = crawl_yesterday_news()

# 7. ì „ì²˜ë¦¬
# ì‹œê°„ëŒ€ ë³€í™˜
## timestamptzì— ì €ì¥í• ë ¤ë©´ UTC+0ìœ¼ë¡œ ë³€í™˜í•´ì•¼í•¨
news_df['publish_date'] = pd.to_datetime(news_df['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')
news_df['publish_date'] = news_df['publish_date'].dt.tz_localize('Asia/Seoul')
news_df['publish_date'] = news_df['publish_date'].dt.tz_convert('UTC')

#################################
# 2. ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§
# ì›¹í¬ë¡¤ë§ëœ ëª¨ë“  htmlì€ news_dfì— ì¡´ì¬í•œë‹¤
def get_news_content(url):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # ìš”ì²­ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì„ íƒ
        # '#articleWrap > div.story-news.article'ì— ê¸°ì‚¬ ë³¸ë¬¸ì´ ì¡´ì¬
        content_container = soup.select_one('#articleWrap > div.story-news.article')
        
            
        if content_container:
            # ì»¨í…Œì´ë„ˆ ì•ˆì˜ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            paragraphs = content_container.select('p')
            # ëª¨ë“  ë¬¸ë‹¨ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            article_text = " ".join([p.get_text(strip=True) for p in paragraphs[:-2]]) # ë§ˆì§€ë§‰ ë‘ ë¬¸ë‹¨ì€ ì œì™¸ # ê¸°ìëª…ê°™ì€ ë¶€ìˆ˜ì ì¸ ë°ì´í„°ê°€ ì¡´ì¬í•¨
            return article_text
        else:
            return pd.NA
            
    except requests.exceptions.RequestException as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {e}"
    
# í•¨ìˆ˜ ì‹¤í–‰
news_df["content"]=news_df.apply(lambda row: get_news_content(row['url']), axis=1)

##################################
# 3. ë‰´ìŠ¤ ìš”ì•½

# ë‰´ìŠ¤ ìš”ì•½ì€ CLOVA X APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•œë‹¤
class CompletionExecutor:
    def __init__(self, host, api_key, request_id):
        self._host = host
        self._api_key = api_key
        self._request_id = request_id

    def execute(self, completion_request):
        """
        APIë¥¼ ì‹¤í–‰í•˜ê³  ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì—ì„œ [DONE] ì§ì „ì˜ ìµœì¢… contentë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        ìì„¸í•œ ì„¤ëª…ì€ 'ì£¼ì‹ ì¶”ì¶œ' í•­ëª© ì°¸ì¡°
        """
        headers = {
            'Authorization': self._api_key,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id,
            'Content-Type': 'application/json; charset=utf-8',
            'Accept': 'text/event-stream'
        }
        
        with requests.post(self._host + '/testapp/v3/chat-completions/HCX-DASH-002',
                           headers=headers, json=completion_request, stream=True) as r:
            
            final_content = None # ìµœì¢… contentë¥¼ ì €ì¥
            
            for line in r.iter_lines():
                if not line:
                    continue

                decoded_line = line.decode('utf-8')
                

                if 'data: [DONE]' in decoded_line:
                    break
                
                if decoded_line.startswith('data:'):
                    try:
                        json_str = decoded_line[len('data:'):].strip()
                        data = json.loads(json_str)
                        if 'message' in data and 'content' in data['message']:
                            if data['message']['content']:
                                final_content = data['message']['content']
                    except (json.JSONDecodeError, KeyError): # ì˜¤ë¥˜ ë°œìƒ ì‹œ
                        continue
                        
        return final_content

def analyze_news_content(content):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë°›ì•„ LLM APIë¡œ ìš”ì•½, ì£¼ìš” ê¸°ì—…, ì´ë²¤íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì²­í•˜ê³  íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìì„¸í•œ ì„¤ëª…ì€ 'ì£¼ì‹ ì¶”ì¶œ' í•­ëª© ì°¸ì¡°
    """
    if not isinstance(content, str) or not content.strip():
        return "ë‚´ìš© ì—†ìŒ"

    # LLM API í˜¸ì¶œì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    api_key_with_bearer = f'Bearer {os.environ.get("CLOVA_API_KEY")}'
    completion_executor = CompletionExecutor(
        host='https://clovastudio.stream.ntruss.com',
        api_key=api_key_with_bearer,
        request_id=os.environ.get("CLOVA_REQUEST_ID")
    )

    # í”„ë¡¬í”„íŠ¸ì™€ ì…ë ¥ ë‚´ìš©
    preset_text = [
        {"role":"system","content":"ë„ˆëŠ” ì¦ê¶Œ ë¶„ì„ê°€ì•¼. ì•„ë˜ ì˜ì–´ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë¶„ì„í•˜ê³ , ê¸ˆìœµ ë° ì¦ê¶Œ ë¶„ì„ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ë‹´ì•„ì„œ í•œêµ­ì–´ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜. (ê¸ì •, ë¶€ì •, ì¤‘ë¦½ì  ë‰˜ì•™ìŠ¤ í¬í•¨)"},
        {"role":"user","content": content}
    ]

    # API ì˜µì…˜
    request_data = {
        'messages': preset_text,
        'topP': 0.8,
        'topK': 0,
        'maxTokens': 256, # ìš”ì•½ ê¸¸ì´ë¥¼ ê³ ë ¤í•œ í† í° ìˆ˜ ì¡°ì •
        'temperature': 0.5,
        'repetitionPenalty': 1.1,
        'stop': [],
        'includeAiFilters': True,
        'seed': 0
    }

    try:
        result = completion_executor.execute(request_data)
        time.sleep(0.5)
        return result if result else "ì‘ë‹µ ì—†ìŒ"

    except Exception as e:
        print(f"API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë‹¨ì¼ ë¬¸ìì—´ ë°˜í™˜
        return "ì˜¤ë¥˜ ë°œìƒ"
    
# í•¨ìˆ˜ ì‹¤í–‰
news_summary = news_df.copy()
news_summary["summary"] = news_summary['content'].apply(analyze_news_content)

################################
# 4. ë²¡í„°í™” (ì„ë² ë”©)
# ë²¡í„°í™”ëŠ” Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰
API_KEY = os.environ.get("GEMINI_API_KEY")

def get_summary_embedding(summary_text: str, client: genai.Client) -> list[float] | None:
    """
    í•˜ë‚˜ì˜ ìš”ì•½ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„ë² ë”© ë²¡í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    ìì„¸í•œ ì„¤ëª…ì€ 'ê¸°ì—… ì„¤ëª… í•œê¸€ë²ˆì—­' í•­ëª© ì°¸ì¡°
    """
    # 1. ìš”ì•½ë³¸ ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if not summary_text or pd.isna(summary_text):
        return None
    
    try:
        # 2. 'contents'ê°€ ì•„ë‹Œ 'content' íŒŒë¼ë¯¸í„°ë¡œ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
        result = client.models.embed_content(
            model="models/text-embedding-004",
            contents=summary_text,
            config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
        )
        # 3. ê²°ê³¼ ê°ì²´ì—ì„œ .embedding ì†ì„±ìœ¼ë¡œ ë²¡í„°ë¥¼ ì§ì ‘ ë°˜í™˜
        vectors = [obj.values for obj in result.embeddings]
        vectors=vectors[0]

        return vectors
    
    except Exception as e:
        print(f"API Error embedding '{summary_text[:50]}...': {e}")
        return None


# API ì‹¤í–‰
if __name__ == "__main__":
    client = genai.Client(api_key=API_KEY)
    news_summary['embedding'] = news_summary['summary'].apply(lambda text: get_summary_embedding(text, client))

#######################################
# 5. Supabaseì— ì €ì¥
supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_KEY")

# ì €ì¥ì— ì‚¬ìš©í•  ë°ì´í„°í”„ë ˆì„
df=news_summary[["title","publish_date","url","summary","embedding"]]

try:
    supabase: Client = create_client(supabase_url, supabase_key)
    print("Supabaseì— ì—°ê²°í•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # ê¸°ì¡´ì˜ ë‰´ìŠ¤ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ì¤‘ë³µì²˜ë¦¬í•˜ì§€ ì•Šê¸° ìœ„í•œ ë¡œì§
    # 1) DBì— ì €ì¥ëœ ëª¨ë“  'title' ëª©ë¡ì„ ê°€ì ¸ì˜¤ê¸°
    response = supabase.table('ko_financial_news_summary').select('title').execute()
    existing_titles = {item['title'] for item in response.data}
    print(f"í˜„ì¬ DBì— {len(existing_titles)}ê°œì˜ ê¸°ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤.")

    # 2) ìƒˆë¡œ ìƒì„±ëœ DataFrameì—ì„œ ì´ë¯¸ ìˆëŠ” titleë“¤ì„ ì œì™¸
    new_articles_df = df[~df['title'].isin(existing_titles)]

    if new_articles_df.empty:
        print("ì¶”ê°€í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"{len(new_articles_df)}ê°œì˜ ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.")

    
        # ì—…ë¡œë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        records_to_upload = new_articles_df.to_dict('records')
        for record in records_to_upload:
            record['publish_date'] = record['publish_date'].isoformat()
        
        # ë°ì´í„° ì‚½ì…
        data, count = supabase.table('ko_financial_news_summary').insert(records_to_upload).execute()
        print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ {len(data[1])}ê°œì˜ ìƒˆ ê¸°ì‚¬ë¥¼ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"âŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
