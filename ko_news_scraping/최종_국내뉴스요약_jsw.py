import os
import pandas as pd
import numpy as np
import time
import json
from datetime import timedelta
import datetime
##ë‰´ìŠ¤ìŠ¤í¬ë©
import re
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from dateutil.parser import parse
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
## API êµ¬ë™
import http.client
import uuid
from supabase import create_client, Client
from google import genai
from google.genai import types
import pytz

##############################
# 1. ì—°í•©ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§

def crawl_yesterday_news():
    """
    ì—°í•©ë‰´ìŠ¤ êµ­ë‚´ì¦ì‹œ ì„¹ì…˜ì—ì„œ ì–´ì œ ì‘ì„±ëœ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # --- 1. ë‚ ì§œ ë° ì‹œê°„ëŒ€ ì„¤ì • ---
    # í•œêµ­ ì‹œê°„ëŒ€(UTC+9)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    tz = pytz.timezone('Asia/Seoul')
    
    # ì–´ì œ ë‚ ì§œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    yesterday = (datetime.datetime.now(tz) - datetime.timedelta(days=1)).date()
    current_year = yesterday.year
    
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {yesterday.strftime('%Y-%m-%d')}")

    # --- 2. í¬ë¡¤ë§ ì¤€ë¹„ ---
    # ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    news_data = []
    
    # --- 3. ì›¹ í˜ì´ì§€ ìˆœíšŒ ë° ë°ì´í„° ì¶”ì¶œ (ì„ íƒì ìˆ˜ì •) ---
    MAX_PAGES_TO_CRAWL = 10  # í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    stop_crawling = False

    for page_num in range(1, MAX_PAGES_TO_CRAWL + 1):
        if stop_crawling:
            break

        url = f'https://www.yna.co.kr/market-plus/domestic-stock/{page_num}'
        print(f"í¬ë¡¤ë§ ì‹œë„... í˜ì´ì§€: {page_num}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # [ìˆ˜ì •] ê¸°ì‚¬ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” div.news-con íƒœê·¸ë¥¼ ëª¨ë‘ ì„ íƒí•©ë‹ˆë‹¤.
            article_containers = soup.select('div.news-con')
            
            # í˜„ì¬ í˜ì´ì§€ì— ê¸°ì‚¬ ì»¨í…Œì´ë„ˆê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
            if not article_containers:
                print(f"í˜ì´ì§€ {page_num}ì—ì„œ ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ(div.news-con)ë¥¼ ì°¾ì§€ ëª»í•´ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

            for article_con in article_containers:
                # ê¸°ì‚¬ ì‹œê°„ê³¼ ì œëª©/ë§í¬ê°€ í¬í•¨ëœ íƒœê·¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
                time_tag = article_con.select_one('span.txt-time')
                title_anchor = article_con.select_one('a.tit-news')

                # í•„ìˆ˜ ìš”ì†Œê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ê¸°ì‚¬ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
                if not time_tag or not title_anchor:
                    continue
                
                time_str = time_tag.get_text(strip=True)
                
                # --- 4. ë‚ ì§œ í•„í„°ë§ (ê°œì„ ëœ ë¡œì§) ---
                try:
                    article_date = datetime.datetime.strptime(f"{current_year}-{time_str}", '%Y-%m-%d %H:%M').date()
                    
                    # ê¸°ì‚¬ ë‚ ì§œê°€ ì–´ì œì™€ ê°™ìœ¼ë©´ ë°ì´í„° ì¶”ê°€
                    if article_date == yesterday:
                        title_span = title_anchor.select_one('span.title01')
                        # ì œëª© íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
                        if not title_span:
                            continue
                            
                        title = title_span.get_text(strip=True)
                        
                        # [ìˆ˜ì •] ì œëª©ì´ '[ì†ë³´]'ë¡œ ì‹œì‘í•˜ëŠ” ê¸°ì‚¬ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
                        if title.startswith('[ì†ë³´]'):
                            continue
                        
                        link = title_anchor['href']
                        
                        datetime_for_db = datetime.datetime.strptime(f"{current_year}-{time_str}", '%Y-%m-%d %H:%M').strftime('%Y-%m-%d %H:%M:%S')
                        
                        news_data.append({
                            'title': title,
                            'date': datetime_for_db,
                            'url': link
                        })
                    
                    # ê¸°ì‚¬ ë‚ ì§œê°€ ì–´ì œë³´ë‹¤ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ëª¨ë“  í˜ì´ì§€ íƒìƒ‰ ì¤‘ë‹¨
                    elif article_date < yesterday:
                        stop_crawling = True
                        break # í˜„ì¬ í˜ì´ì§€ì˜ ë‚˜ë¨¸ì§€ ê¸°ì‚¬ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨

                except ValueError:
                    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ í•´ë‹¹ ê¸°ì‚¬ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
                    continue
            
            if stop_crawling:
                print(f"í˜ì´ì§€ {page_num}ì—ì„œ ëŒ€ìƒ ë‚ ì§œë³´ë‹¤ ì˜¤ë˜ëœ ê¸°ì‚¬ë¥¼ ë°œê²¬í•˜ì—¬ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

        except requests.exceptions.RequestException as e:
            print(f"í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url}, ì˜¤ë¥˜: {e}")
            break
            
    # --- 5. ë°ì´í„°í”„ë ˆì„ ìƒì„± ---
    if not news_data:
        print("ìµœëŒ€ í˜ì´ì§€ê¹Œì§€ íƒìƒ‰í–ˆì§€ë§Œ ì–´ì œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
        
    df = pd.DataFrame(news_data)
    
    return df

# --- 6. í•¨ìˆ˜ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥ ---
news_df = crawl_yesterday_news()

# ì‹œê°„ëŒ€ ë³€í™˜
## timestamptzì— ì €ì¥í• ë ¤ë©´ UTC+0ìœ¼ë¡œ ë³€í™˜í•´ì•¼í•¨
news_df['publish_date'] = pd.to_datetime(news_df['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')
news_df['publish_date'] = news_df['publish_date'].dt.tz_localize('Asia/Seoul')
news_df['publish_date'] = news_df['publish_date'].dt.tz_convert('UTC')

#################################33
# 2. ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§
def get_news_content(url):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # ìš”ì²­ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì„ íƒ
        # ì¼ë¶€ ì–¸ë¡ ì‚¬ëŠ” ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” '#articleWrap > div.story-news.article'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        content_container = soup.select_one('#articleWrap > div.story-news.article')
        
            
        if content_container:
            # ì»¨í…Œì´ë„ˆ ì•ˆì˜ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            paragraphs = content_container.select('p')
            # ëª¨ë“  ë¬¸ë‹¨ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.
            article_text = " ".join([p.get_text(strip=True) for p in paragraphs[:-2]]) # ë§ˆì§€ë§‰ ë‘ ë¬¸ë‹¨ì€ ì œì™¸
            return article_text
        else:
            return pd.NA
            
    except requests.exceptions.RequestException as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {e}"
    
# í•¨ìˆ˜ ì‹¤í–‰
news_df["content"]=news_df.apply(lambda row: get_news_content(row['url']), axis=1)

##################################
# 3. ë‰´ìŠ¤ ìš”ì•½

# ë‰´ìŠ¤ ìš”ì•½ì€ CLOVA X APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•œë‹¤.
## ë”°ë¼ì„œ ê³µì‹ìœ¼ë¡œ ì œê³µë˜ëŠ” ì˜ˆì œë¥¼ ìˆ˜ì •í•˜ì—¬ í´ë˜ìŠ¤ë¥¼ ë¯¸ë¦¬ ìƒì„±í•´ì•¼í•œë‹¤.
class CompletionExecutor:
    def __init__(self, host, api_key, request_id):
        self._host = host
        self._api_key = api_key
        self._request_id = request_id

    def execute(self, completion_request):
        """
        APIë¥¼ ì‹¤í–‰í•˜ê³  ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì—ì„œ [DONE] ì§ì „ì˜ ìµœì¢… contentë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        headers = {
            'Authorization': self._api_key,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id,
            'Content-Type': 'application/json; charset=utf-8',
            'Accept': 'text/event-stream'
        }
        
        # ì›ë˜ ì‚¬ìš©í•˜ë˜ ì—”ë“œí¬ì¸íŠ¸
        with requests.post(self._host + '/testapp/v3/chat-completions/HCX-DASH-002',
                           headers=headers, json=completion_request, stream=True) as r:
            
            final_content = None # ìµœì¢… contentë¥¼ ì €ì¥í•  ë³€ìˆ˜
            
            for line in r.iter_lines():
                if not line:
                    continue

                decoded_line = line.decode('utf-8')
                
                # ìŠ¤íŠ¸ë¦¼ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” '[DONE]' ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                ## ê°ê°ì˜ í† ê·¼ì„ ë°›ì•„ì˜¤ëŠ” í˜•íƒœì´ê³ , ë§ˆì§€ë§‰ì— ì´ ê²°ê³¼ë¥¼ ì¢…í•©í•œê²ƒì„ ë”°ë¡œ ì¶œë ¥í•œë‹¤.
                if 'data: [DONE]' in decoded_line:
                    break
                
                if decoded_line.startswith('data:'):
                    try:
                        # ë‚´ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ê²°ê³¼ë¥¼ jsonìœ¼ë¡œ ë°˜í™˜í•˜ë¼ê³  í•˜ì˜€ë‹¤.
                        ## ë”°ë¼ì„œ json íŒŒì‹±ì„ ì§„í–‰í•œë‹¤.
                        json_str = decoded_line[len('data:'):].strip()
                        data = json.loads(json_str)
                        
                        # message ê°ì²´ì™€ contentê°€ ìˆëŠ”ì§€ í™•ì¸
                        if 'message' in data and 'content' in data['message']:
                            # contentê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°, ì´ ë‚´ìš©ì„ ìµœì¢… ê²°ê³¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.
                            # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ë§ˆì§€ë§‰ ìœ íš¨í•œ contentê°€ ìµœì¢… ê²°ê³¼ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
                            if data['message']['content']:
                                final_content = data['message']['content']
                    except (json.JSONDecodeError, KeyError):
                        # íŒŒì‹± ì˜¤ë¥˜ë‚˜ í‚¤ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ í•´ë‹¹ ë¼ì¸ì€ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                        continue
                        
        return final_content

def analyze_news_content(content):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë°›ì•„ LLM APIë¡œ ìš”ì•½, ì£¼ìš” ê¸°ì—…, ì´ë²¤íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì²­í•˜ê³  íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not isinstance(content, str) or not content.strip():
        return "ë‚´ìš© ì—†ìŒ"

    # LLM API í˜¸ì¶œì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    completion_executor = CompletionExecutor(
        host='https://clovastudio.stream.ntruss.com',
        api_key='Bearer nv-3da49e910a8b4d3b99721dab49141e35xINY', # ì´ê²ƒì€ í…ŒìŠ¤íŠ¸í‚¤ì´ë¯€ë¡œ ë‚˜ì¤‘ì— ì‚¬ì—…ìš© í‚¤ë¥¼ ë°›ì•˜ì„ ê²½ìš° ëŒ€ì²´í•œë‹¤.
        request_id='b006c788142a49a9adf34e73d6b0a403' # APIë¥¼ êµ¬ë³„í•´ì£¼ëŠ” ID
    )

    # APIì— ì „ë‹¬í•  ë©”ì‹œì§€ êµ¬ì„± (JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ìš”ì²­)
    preset_text = [
        # í”„ë¡¬í”„íŠ¸
        {"role":"system","content":"ë„ˆëŠ” ì¦ê¶Œ ë¶„ì„ê°€ì•¼. ì•„ë˜ ì˜ì–´ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë¶„ì„í•˜ê³ , ê¸ˆìœµ ë° ì¦ê¶Œ ë¶„ì„ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ë‹´ì•„ì„œ í•œêµ­ì–´ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜. (ê¸ì •, ë¶€ì •, ì¤‘ë¦½ì  ë‰˜ì•™ìŠ¤ í¬í•¨)"},
        # ì…ë ¥í•  ë°ì´í„° ë‚´ìš©
        {"role":"user","content": content}
    ]

    # API ì˜µì…˜ (ê¸°ë³¸ê°’ ì‚¬ìš©)
    request_data = {
        'messages': preset_text,
        'topP': 0.8,
        'topK': 0,
        'maxTokens': 256, # ìš”ì•½ ê¸¸ì´ë¥¼ ê³ ë ¤í•˜ì—¬ í† í° ìˆ˜ ì¦ê°€
        'temperature': 0.5,
        'repetitionPenalty': 1.1,
        'stop': [],
        'includeAiFilters': True,
        'seed': 0
    }

    try:
        result = completion_executor.execute(request_data)
        time.sleep(0.5)
        
        # ì„±ê³µ ì‹œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë„ë¡ return ì¶”ê°€
        return result if result else "ì‘ë‹µ ì—†ìŒ"

    except Exception as e:
        print(f"API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë‹¨ì¼ ë¬¸ìì—´ ë°˜í™˜
        return "ì˜¤ë¥˜ ë°œìƒ"
    
# í•¨ìˆ˜ ì‹¤í–‰
news_summary = news_df.copy()

# pd.Seriesì™€ lambdaë¥¼ ì œê±°í•˜ê³  í•¨ìˆ˜ë¥¼ ì§ì ‘ ì ìš©í•©ë‹ˆë‹¤.
news_summary["summary"] = news_summary['content'].apply(analyze_news_content)

################################
# 4. ë²¡í„°í™” (ì„ë² ë”©)
API_KEY = os.environ.get("GEMINI_API_KEY")

def get_summary_embedding(summary_text: str, client: genai.Client) -> list[float] | None:
    """
    í•˜ë‚˜ì˜ ìš”ì•½ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„ë² ë”© ë²¡í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    """
    # 1. ìš”ì•½ë³¸ ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if not summary_text or pd.isna(summary_text):
        return None
    
    try:
        # 2. 'contents'ê°€ ì•„ë‹Œ 'content' íŒŒë¼ë¯¸í„°ë¡œ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
        result = client.models.embed_content(
            model="models/text-embedding-004",
            contents=summary_text,
            config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
        )
        # 3. ê²°ê³¼ ê°ì²´ì—ì„œ .embedding ì†ì„±ìœ¼ë¡œ ë²¡í„°ë¥¼ ì§ì ‘ ë°˜í™˜
        vectors = [obj.values for obj in result.embeddings]
        vectors=vectors[0]

        return vectors
    
    except Exception as e:
        print(f"API Error embedding '{summary_text[:50]}...': {e}")
        return None


# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    # í´ë¼ì´ì–¸íŠ¸ëŠ” í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤.
    client = genai.Client(api_key=API_KEY)
    news_summary['embedding'] = news_summary['summary'].apply(lambda text: get_summary_embedding(text, client))

#######################################
# 5. Supabaseì— ì €ì¥
supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_KEY")

#ì €ì¥ì— ì‚¬ìš©í•  ë°ì´í„°í”„ë ˆì„
df=news_summary[["title","publish_date","url","summary","embedding"]]

try:
    supabase: Client = create_client(supabase_url, supabase_key)
    print("â˜ï¸ Supabaseì— ì—°ê²°í•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # DBì— ì €ì¥ëœ ëª¨ë“  'title' ëª©ë¡ì„ ê°€ì ¸ì˜¤ê¸°
    response = supabase.table('ko_financial_news_summary').select('title').execute()
    existing_titles = {item['title'] for item in response.data}
    print(f"í˜„ì¬ DBì— {len(existing_titles)}ê°œì˜ ê¸°ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤.")

    # ìƒˆë¡œ ìƒì„±ëœ DataFrameì—ì„œ ì´ë¯¸ ìˆëŠ” titleë“¤ì„ ì œì™¸
    new_articles_df = df[~df['title'].isin(existing_titles)]

    if new_articles_df.empty:
        print("âœ… ì¶”ê°€í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ¨ {len(new_articles_df)}ê°œì˜ ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.")

    
        # ì—…ë¡œë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        records_to_upload = new_articles_df.to_dict('records')
        for record in records_to_upload:
            record['publish_date'] = record['publish_date'].isoformat()
        
        # ë°ì´í„° ì‚½ì…
        data, count = supabase.table('ko_financial_news_summary').insert(records_to_upload).execute()
        print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ {len(data[1])}ê°œì˜ ìƒˆ ê¸°ì‚¬ë¥¼ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"âŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
